{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test \"Multi-Label Confusion Matrix\" (MLCM) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Please read the following paper for more information:\n",
    "#     M. Heydarian, T. Doyle, and R. Samavi, MLCM: Multi-Label Confusion Matrix, \n",
    "#     IEEE Access, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "from MLCM import mlcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating random True and Predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating \"random true\" and \"random predicted\" labels (multi-label); \n",
    "# 1000 samples of 5 classes.\n",
    "number_of_samples = 1000\n",
    "number_of_classes = 5\n",
    "label_true = np.random.randint(2, size=(number_of_samples, number_of_classes))\n",
    "label_pred = np.random.randint(2, size=(number_of_samples, number_of_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples of True labels\n",
      " [[1 0 0 1 1]\n",
      " [1 1 0 1 1]\n",
      " [0 1 0 0 1]\n",
      " [0 0 0 0 1]]\n",
      "examples of Predicted labels\n",
      " [[0 1 1 0 0]\n",
      " [1 1 1 1 0]\n",
      " [1 1 1 0 1]\n",
      " [0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print('examples of True labels\\n',label_true[:4])\n",
    "print('examples of Predicted labels\\n',label_pred[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scores using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[249 251]\n",
      "  [254 246]]\n",
      "\n",
      " [[252 244]\n",
      "  [252 252]]\n",
      "\n",
      " [[233 273]\n",
      "  [266 228]]\n",
      "\n",
      " [[241 252]\n",
      "  [268 239]]\n",
      "\n",
      " [[263 231]\n",
      "  [238 268]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.49      0.49       500\n",
      "           1       0.51      0.50      0.50       504\n",
      "           2       0.46      0.46      0.46       494\n",
      "           3       0.49      0.47      0.48       507\n",
      "           4       0.54      0.53      0.53       506\n",
      "\n",
      "   micro avg       0.50      0.49      0.49      2511\n",
      "   macro avg       0.50      0.49      0.49      2511\n",
      "weighted avg       0.50      0.49      0.49      2511\n",
      " samples avg       0.48      0.48      0.44      2511\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/lib/python3/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cm = skm.multilabel_confusion_matrix(label_true,label_pred)\n",
    "print(cm)\n",
    "print(skm.classification_report(label_true,label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing confusion matrix using 'MLCM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLCM has one extra row (NTL) and one extra column (NPL).        \n",
      "Please read the following paper for more information:\n",
      "        Heydarian et al., MLCM: Multi-Label Confusion Matrix, IEEE Access,2022        \n",
      "To skip this message, please add parameter \"print_note=False\"\n",
      "        e.g., conf_mat,normal_conf_mat = mlcm.cm(label_true,label_pred,False)\n",
      "\n",
      "Raw confusion Matrix:\n",
      "[[246  96 104 102  92  69]\n",
      " [ 89 252 114 114 103  72]\n",
      " [ 91  88 228 111  99  75]\n",
      " [ 92 114 115 239  91  88]\n",
      " [ 95  84 112 113 268  72]\n",
      " [ 13  12  14  10  13   1]]\n",
      "\n",
      "Normalized confusion Matrix (%):\n",
      "[[35. 14. 15. 14. 13. 10.]\n",
      " [12. 34. 15. 15. 14. 10.]\n",
      " [13. 13. 33. 16. 14. 11.]\n",
      " [12. 15. 16. 32. 12. 12.]\n",
      " [13. 11. 15. 15. 36. 10.]\n",
      " [21. 19. 22. 16. 21.  2.]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat,normal_conf_mat = mlcm.cm(label_true,label_pred)\n",
    "\n",
    "print('\\nRaw confusion Matrix:')\n",
    "print(conf_mat)\n",
    "print('\\nNormalized confusion Matrix (%):')\n",
    "print(normal_conf_mat) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scores using MLCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 988  380]\n",
      "  [ 463  246]]\n",
      "\n",
      " [[ 982  394]\n",
      "  [ 492  252]]\n",
      "\n",
      " [[1006  459]\n",
      "  [ 464  228]]\n",
      "\n",
      " [[ 995  450]\n",
      "  [ 500  239]]\n",
      "\n",
      " [[ 966  398]\n",
      "  [ 476  268]]\n",
      "\n",
      " [[1233  376]\n",
      "  [  62    1]]]\n",
      "\n",
      "       class#     precision        recall      f1-score        weight\n",
      "\n",
      "            0          0.39          0.35          0.37          709\n",
      "            1          0.39          0.34          0.36          744\n",
      "            2          0.33          0.33          0.33          692\n",
      "            3          0.35          0.32          0.33          739\n",
      "            4          0.40          0.36          0.38          744\n",
      "          NTL          0.00          0.02          0.00          63\n",
      "\n",
      "    micro avg          0.33          0.33          0.33          3691\n",
      "    macro avg          0.31          0.29          0.30          3691\n",
      " weighted avg          0.37          0.33          0.35          3691\n"
     ]
    }
   ],
   "source": [
    "one_vs_rest = mlcm.stats(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       class#     precision        recall      f1-score        weight\n",
      "\n",
      "            0          0.39          0.35          0.37          709\n",
      "            1          0.39          0.34          0.36          744\n",
      "            2          0.33          0.33          0.33          692\n",
      "            3          0.35          0.32          0.33          739\n",
      "            4          0.40          0.36          0.38          744\n",
      "          NTL          0.00          0.02          0.00          63\n",
      "\n",
      "    micro avg          0.33          0.33          0.33          3691\n",
      "    macro avg          0.31          0.29          0.30          3691\n",
      " weighted avg          0.37          0.33          0.35          3691\n"
     ]
    }
   ],
   "source": [
    "one_vs_rest = mlcm.stats(conf_mat, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[[ 988  380]\n",
      "  [ 463  246]]\n",
      "\n",
      " [[ 982  394]\n",
      "  [ 492  252]]\n",
      "\n",
      " [[1006  459]\n",
      "  [ 464  228]]\n",
      "\n",
      " [[ 995  450]\n",
      "  [ 500  239]]\n",
      "\n",
      " [[ 966  398]\n",
      "  [ 476  268]]\n",
      "\n",
      " [[1233  376]\n",
      "  [  62    1]]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n',one_vs_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
